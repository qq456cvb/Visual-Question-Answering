{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import collections\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.trainer as trainer\n",
    "import torch.utils.trainer.plugins\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = models.resnet152(pretrained=True)\n",
    "use_cuda = True\n",
    "# print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine2D(nn.Module):\n",
    "    def __init__(self, L, M, N):\n",
    "        super(Affine2D, self).__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(M, N))\n",
    "        self.bias = nn.Parameter(torch.Tensor(L, N))\n",
    "        \n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.bias.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        self.L = L\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        \n",
    "    #   expect a 3d tensor : Batch * L * M\n",
    "    def forward(self, input):\n",
    "        input = input.contiguous().view(-1, self.M)\n",
    "        input = torch.mm(input, self.weight)\n",
    "        input = input.view(-1, self.L, self.N)\n",
    "        input += self.bias.unsqueeze(0).expand_as(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoW(nn.Module):\n",
    "    def __init__(self, T, N):\n",
    "        super(BoW, self).__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(1, T))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1, N))\n",
    "        \n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.bias.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        \n",
    "    #   expect a 3d sentence tensor : Batch * T * N\n",
    "    def forward(self, input):\n",
    "        output = autograd.Variable(torch.Tensor(input.size()[0], input.size()[2]))\n",
    "        for b in xrange(input.size()[0]):\n",
    "            output[b, :] = (torch.mm(self.weight, input[b, :, :]) + self.bias).squeeze()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SMN(nn.Module):\n",
    "    def __init__(self, model, hop, L, T, M, N, K, dic_size):\n",
    "        super(SMN, self).__init__()\n",
    "\n",
    "        # load resnet\n",
    "        modules = list(model.children())\n",
    "        modules.pop()\n",
    "        modules.pop()\n",
    "        modules.pop()\n",
    "        self.features = nn.Sequential(*modules)\n",
    "        for param in self.features.parameters():\n",
    "            param.require_grad = False\n",
    "\n",
    "        self.L = L\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.dic_size = dic_size\n",
    "        \n",
    "        self.hop = hop\n",
    "        \n",
    "        # word embedding\n",
    "        self.embedding = nn.Embedding(self.dic_size, self.N)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            Affine2D(self.L, self.M, self.N)\n",
    "        )\n",
    "        self.evidence = nn.Sequential(\n",
    "            Affine2D(self.L, self.M, self.N)\n",
    "        )\n",
    "        self.BoW = BoW(self.T, self.N)\n",
    "        self.linear = nn.Linear(self.N, self.K)\n",
    "        \n",
    "        \n",
    "    def forward(self, img, question):\n",
    "        #   extract features from resNet\n",
    "        S = self.features(img)\n",
    "        \n",
    "        # embedding\n",
    "        V = self.embedding(question)\n",
    "        \n",
    "        #   word attention\n",
    "        S = ((S.view(-1, self.M, self.L)).permute(0, 2, 1))\n",
    "        att = self.attention(S)\n",
    "        \n",
    "        batch_size = att.size()[0]\n",
    "        C = autograd.Variable(torch.FloatTensor(batch_size, self.T, self.L))\n",
    "        C = C.cuda()\n",
    "        for b in xrange(batch_size):\n",
    "            C[b, :, :] = torch.mm(V[b, :, :], att.transpose(1, 2)[b, :, :])\n",
    "\n",
    "        # C : Batch * T * L\n",
    "        W_att = nn.Softmax()(torch.squeeze(torch.max(C, 1)[0])).unsqueeze(1)\n",
    "    \n",
    "        # evidence * attention\n",
    "        evid = self.evidence(S)\n",
    "        S_att = autograd.Variable(torch.Tensor(batch_size, self.N))\n",
    "        for b in xrange(batch_size):\n",
    "            S_att[b, :] = torch.mm(W_att[b, :, :], evid[b, :, :]).squeeze()\n",
    "        \n",
    "        Q = self.BoW(V)\n",
    "        \n",
    "        if self.hop == 1:    \n",
    "            # 1-hop output\n",
    "            P = nn.LogSoftmax()(self.linear(nn.ReLU()(S_att + Q)))\n",
    "        elif self.hop == 2:\n",
    "            O_hop1 = S_att + Q\n",
    "            C_hop2 = autograd.Variable(torch.Tensor(batch_size, self.L))\n",
    "            for b in xrange(batch_size):\n",
    "                C_hop2[b, :] = torch.mm(evid[b, :, :], O_hop1[b, :].unsqueeze(1)).squeeze()\n",
    "            \n",
    "            W_att2 = nn.Softmax()(C_hop2).unsqueeze(1)\n",
    "            evid2 = self.evidence(S)\n",
    "            S_att2 = autograd.Variable(torch.Tensor(batch_size, self.N))\n",
    "            for b in xrange(batch_size):\n",
    "                S_att2[b, :] = torch.mm(W_att2[b, :, :], evid2[b, :, :]).squeeze()\n",
    "            \n",
    "            P = nn.LogSoftmax()(self.linear(nn.ReLU()(O_hop1 + S_att2)))\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3876\n"
     ]
    }
   ],
   "source": [
    "stc_idx_to_img_name = {}\n",
    "question_word_to_idx = {'UNK': 0, 'END': 1}\n",
    "answer_word_to_idx = {'UNK': 0}\n",
    "\n",
    "# read txt questions and answers\n",
    "f = open('./datasets/texts/train/train.txt', 'r')\n",
    "\n",
    "# preload\n",
    "is_question = True\n",
    "line = f.readline()\n",
    "max_len = 0\n",
    "cnt = 0\n",
    "while line != '':\n",
    "    if is_question:\n",
    "        cnt = cnt + 1\n",
    "        if len(line.split()[:-4]) > max_len:\n",
    "            max_len = len(line.split()[:-4])\n",
    "        for word in line.split()[:-4]:\n",
    "            if word not in question_word_to_idx:\n",
    "                question_word_to_idx[word] = len(question_word_to_idx)\n",
    "    else:\n",
    "        word = line.split()[0]\n",
    "        if word not in answer_word_to_idx:\n",
    "            answer_word_to_idx[word] = len(answer_word_to_idx)\n",
    "    line = f.readline()\n",
    "    is_question = not is_question\n",
    "    \n",
    "\n",
    "# reload, add padding and convert into integers\n",
    "f.seek(0)\n",
    "line = f.readline()\n",
    "train_q = torch.LongTensor(cnt, max_len).zero_() + 1\n",
    "train_a = torch.LongTensor(cnt)\n",
    "train_img_names = []\n",
    "idx = 0\n",
    "while line != '':\n",
    "    if is_question:\n",
    "        q = line.split()[:-4]\n",
    "        train_img_names.append(line.split()[-2])\n",
    "        for i in xrange(len(q)):\n",
    "            train_q[idx, i] = question_word_to_idx[q[i]]\n",
    "    else:\n",
    "        word = line.split()[0]\n",
    "        train_a[idx] = answer_word_to_idx[word]\n",
    "        idx = idx + 1\n",
    "    line = f.readline()\n",
    "    is_question = not is_question\n",
    "\n",
    "f.close()\n",
    "\n",
    "# print max_len\n",
    "# print len(question_word_to_idx)\n",
    "# print torch.max(train_q)\n",
    "# print len(train_img_names)\n",
    "# print len(answer_word_to_idx)\n",
    "\n",
    "\n",
    "smn_model = SMN(model, \n",
    "                2,\n",
    "                L=14*14, \n",
    "                T=max_len, \n",
    "                M=1024, \n",
    "                N=300, \n",
    "                K=len(answer_word_to_idx), \n",
    "                dic_size=len(question_word_to_idx)\n",
    "                )\n",
    "\n",
    "if use_cuda:\n",
    "    smn_model = torch.nn.DataParallel(smn_model).cuda()\n",
    "\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "f = open('./datasets/texts/val/val.txt', 'r')\n",
    "\n",
    "cnt = 0\n",
    "line = f.readline()\n",
    "while line != '':\n",
    "    cnt = cnt + 1\n",
    "    line = f.readline()\n",
    "\n",
    "cnt /= 2\n",
    "f.seek(0)\n",
    "line = f.readline()\n",
    "val_q = torch.LongTensor(cnt, max_len).zero_() + 1\n",
    "val_a = torch.LongTensor(cnt)\n",
    "val_img_names = []\n",
    "idx = 0\n",
    "is_question = True\n",
    "while line != '':\n",
    "    if is_question:\n",
    "        q = line.split()[:-4]\n",
    "        val_img_names.append(line.split()[-2])\n",
    "        for i in xrange(len(q)):\n",
    "            if q[i] not in question_word_to_idx:\n",
    "                val_q[idx, i] = 0\n",
    "            else:\n",
    "                val_q[idx, i] = question_word_to_idx[q[i]]\n",
    "    else:\n",
    "        word = line.split()[0]\n",
    "        if word not in answer_word_to_idx:\n",
    "            val_a[idx] = 0\n",
    "        else:\n",
    "            val_a[idx] = answer_word_to_idx[word]\n",
    "        idx = idx + 1\n",
    "    line = f.readline()\n",
    "    is_question = not is_question\n",
    "\n",
    "f.close()\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, questions, answers, imgs, transform=None):\n",
    "        super(VQADataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename + '.png'))\n",
    "        img = img.resize((224, 224))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.questions[index], self.answers[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = VQADataset('./datasets/images',\n",
    "                          train_q,\n",
    "                          train_a,\n",
    "                          train_img_names,\n",
    "                          transforms.Compose([\n",
    "#                                 transforms.Scale(224),\n",
    "#                                 transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "                            ])\n",
    "                         )\n",
    "val_data = VQADataset('./datasets/images',\n",
    "                          val_q,\n",
    "                          val_a,\n",
    "                          val_img_names,\n",
    "                          transforms.Compose([\n",
    "#                                 transforms.Scale(224),\n",
    "#                                 transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "                            ])\n",
    "                         )\n",
    "\n",
    "# print train_data.__getitem__(2)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=2, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageLogger:\n",
    "    def __init__(self):\n",
    "        self.N = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val += val\n",
    "        self.N += n\n",
    "        self.avg = self.val / self.N\n",
    "    def getAverage(self):\n",
    "        return self.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    pred_max, indices = torch.sort(pred, dim=1, descending=True)\n",
    "    indices = torch.squeeze(indices[:, 0])\n",
    "    return float((indices == target).sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageLogger()\n",
    "    acc = AverageLogger()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    old_time= time.time()\n",
    "    for i, (images, questions, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "\n",
    "        # compute y_pred\n",
    "        y_pred = model(autograd.Variable(images), autograd.Variable(questions))\n",
    "        loss = criterion(y_pred, autograd.Variable(target))\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "#         print y_pred.size()\n",
    "#         print target.size()\n",
    "        pred_acc = accuracy(y_pred.data, target)\n",
    "        losses.update(loss.data[0], y_pred.size(0))\n",
    "        acc.update(pred_acc, y_pred.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        new_time = time.time()\n",
    "        print \"Batch %d's time: %.3f\" % (i, new_time - old_time)\n",
    "        old_time = new_time\n",
    "        break\n",
    "    \n",
    "    print \"Epoch %d: Losses: %.3f, Accuracy: %.3f\" % (epoch, losses.getAverage(), acc.getAverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.addmm received an invalid combination of arguments - got (int, torch.FloatTensor, int, torch.FloatTensor, torch.cuda.FloatTensor, out=torch.FloatTensor), but expected one of:\n * (torch.FloatTensor source, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (torch.FloatTensor source, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (float beta, torch.FloatTensor source, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (torch.FloatTensor source, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (float beta, torch.FloatTensor source, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (torch.FloatTensor source, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (float beta, torch.FloatTensor source, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.FloatTensor\u001b[0m)\n * (float beta, torch.FloatTensor source, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.FloatTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-56a408a26e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-5fc97fb5a1ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# compute y_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-27f308bc2c70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, question)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mS_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mS_att\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_att\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_static_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_static_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_args\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, add_matrix, matrix1, matrix2)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         return torch.addmm(self.alpha, add_matrix, self.beta,\n\u001b[0;32m---> 28\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.addmm received an invalid combination of arguments - got (int, torch.FloatTensor, int, torch.FloatTensor, torch.cuda.FloatTensor, out=torch.FloatTensor), but expected one of:\n * (torch.FloatTensor source, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (torch.FloatTensor source, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (float beta, torch.FloatTensor source, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (torch.FloatTensor source, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (float beta, torch.FloatTensor source, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (torch.FloatTensor source, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n * (float beta, torch.FloatTensor source, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.FloatTensor\u001b[0m)\n * (float beta, torch.FloatTensor source, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2, *, torch.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mout=torch.FloatTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "if use_cuda:\n",
    "    criterion = criterion.cuda()\n",
    "optimizer = torch.optim.SGD(smn_model.parameters(), 1e-3, 0.9)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in xrange(epochs):\n",
    "    train(train_loader, smn_model, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
